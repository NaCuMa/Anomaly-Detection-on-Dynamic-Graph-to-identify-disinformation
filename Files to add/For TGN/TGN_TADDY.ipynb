{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee634f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c081c6f",
   "metadata": {},
   "source": [
    "### Dataset selection and anomaly generation through TADDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb22bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data with anomaly for Dataset:  btc_alpha\n",
      "Preprocess dataset: btc_alpha\n",
      "vertex: 3783  edge:  14124\n",
      "Preprocess finished! Time: 0.18 s\n",
      "[#s] generating anomalous dataset...\n",
      " 2022-08-31 15:18:29.573942\n",
      "[#s] initial network edge percent: #.1f##, anomaly percent: #.1f##.\n",
      " 2022-08-31 15:18:29.573942 50.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geode\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:245: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "70\n",
      "Anomaly Generation finish! Time: 7.98 s\n",
      "Train size:7062  7  Test size:7132 7\n",
      "Training dataset contruction finish! Time: 0.01 s\n",
      "Test dataset finish constructing! Time: 0.00 s\n",
      "Generating data with anomaly for Dataset:  btc_alpha\n",
      "[#s] generating anomalous dataset...\n",
      " 2022-08-31 15:18:37.628780\n",
      "[#s] initial network edge percent: #.1f##, anomaly percent: #.1f##.\n",
      " 2022-08-31 15:18:37.633842 50.0 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geode\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:245: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "353\n",
      "Anomaly Generation finish! Time: 7.91 s\n",
      "Train size:7062  7  Test size:7415 7\n",
      "Training dataset contruction finish! Time: 0.01 s\n",
      "Test dataset finish constructing! Time: 0.00 s\n",
      "Generating data with anomaly for Dataset:  btc_alpha\n",
      "[#s] generating anomalous dataset...\n",
      " 2022-08-31 15:18:45.624318\n",
      "[#s] initial network edge percent: #.1f##, anomaly percent: #.1f##.\n",
      " 2022-08-31 15:18:45.624318 50.0 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geode\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:245: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n",
      "706\n",
      "Anomaly Generation finish! Time: 7.85 s\n",
      "Train size:7062  7  Test size:7768 8\n",
      "Training dataset contruction finish! Time: 0.01 s\n",
      "Test dataset finish constructing! Time: 0.00 s\n"
     ]
    }
   ],
   "source": [
    "# Change the dataset here\n",
    "data_set = \"btc_alpha\"\n",
    "\n",
    "# Here too\n",
    "%run ../TADDY/0_prepare_data.py --dataset btc_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3eb4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'uci':'uci',\n",
    "             'digg':'digg',\n",
    "             'btc_alpha':'soc-sign-bitcoinalpha',\n",
    "             'btc_otc':'soc-sign-bitcoinotc',\n",
    "             'email':'email-dnc',\n",
    "             'AST':'AST',\n",
    "             'TGN':'TGN'}\n",
    "data_file = data_files[data_set]\n",
    "if data_set in ['digg', 'uci', 'AST']:\n",
    "    data = np.loadtxt(f\"C:/Users/geode/TADDY/data/raw/\"+data_file, dtype=float, comments='%', delimiter=' ')\n",
    "elif data_set in ['btc_alpha', 'btc_otc', 'email', 'TGN']:\n",
    "    data = pd.read_csv(f\"../TADDY/data/raw/{data_file}.csv\", sep=',', header=None).to_numpy()\n",
    "\n",
    "nodes = np.unique(np.concatenate((data.T[0], data.T[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4912d6",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78fc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nop_TADDY(data, num_anom_edges):\n",
    "    data = np.vstack((np.hstack((data[:-num_anom_edges], np.full((data.shape[0]-num_anom_edges, 1), 0))),\n",
    "                    np.hstack((data[-num_anom_edges:], np.full((num_anom_edges, 1), 1)))))\n",
    "    \n",
    "    data = np.array(data).T\n",
    "    data[2] -= np.min(data[2])\n",
    "    data = data.T\n",
    "    ind = np.argsort(data[:,2])\n",
    "    data = data[ind]\n",
    "    data = data.T\n",
    "\n",
    "    indices = np.array([data[0],data[1]]).reshape((1,2*data.shape[1]))\n",
    "    indices = np.unique(indices)\n",
    "    dict_indices = {}\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        dict_indices[indices[i]] = i\n",
    "\n",
    "    for i in range(len(data.T)):\n",
    "        data[0][i] = dict_indices[data[0][i]]\n",
    "        data[1][i] = dict_indices[data[1][i]]\n",
    "\n",
    "    data = data.T\n",
    "\n",
    "    data_final = np.zeros((data.shape[0]+len(indices), 173))\n",
    "    data_final[:len(indices)] = np.hstack((np.full((len(indices),1),-1), np.full((len(indices),172),1)))\n",
    "    data_final[len(indices):] = np.hstack((data.T[:3].T, np.full((data.shape[0],170),-1)))\n",
    "\n",
    "    data_final = pd.DataFrame(data_final)\n",
    "\n",
    "    return data_final, np.where(data.T[3]==1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c549c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snaps_final_indices(data, data_nop, snap_indices, data_train):\n",
    "    snaps_times = data[len(data_train)+snap_indices-1].reshape(len(snap_indices), 3)[:, 2:]-min(data[:,2:])#.astype(np.int32)-int(min(data[:,2:]))\n",
    "    print(\"The last training instant for this dataset is: \", snaps_times[0][0])\n",
    "    data_nop = np.array(data_nop)\n",
    "    start_interactions = np.where(data_nop.T[0] != -1)[0][0]\n",
    "    data_nop = data_nop[start_interactions:]\n",
    "    data_nop = data_nop[:,0:3]\n",
    "    final_indices = []\n",
    "\n",
    "    for time in snaps_times:\n",
    "        final_indices.append(np.where(data_nop.T[2] == time)[0][0])\n",
    "    return final_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e838f19",
   "metadata": {},
   "source": [
    "### Create data for TGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d58c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(f\"../TADDY/data/for_TGN/train_{data_set}_0.5_0.01.csv\", sep=',').to_numpy()\n",
    "data_test_001 = pd.read_csv(f\"../TADDY/data/for_TGN/test_{data_set}_0.5_0.01.csv\", sep=',').to_numpy()\n",
    "snap_indices_001 = pd.read_csv(f\"../TADDY/data/for_TGN/{data_set}_0.5_0.01_snap_indices.csv\")\n",
    "data_test_005 = pd.read_csv(f\"../TADDY/data/for_TGN/test_{data_set}_0.5_0.05.csv\", sep=',').to_numpy()\n",
    "snap_indices_005 = pd.read_csv(f\"../TADDY/data/for_TGN/{data_set}_0.5_0.05_snap_indices.csv\")\n",
    "data_test_01 = pd.read_csv(f\"../TADDY/data/for_TGN/test_{data_set}_0.5_0.1.csv\", sep=',').to_numpy()\n",
    "snap_indices_01 = pd.read_csv(f\"../TADDY/data/for_TGN/{data_set}_0.5_0.1_snap_indices.csv\")\n",
    "data_times = pd.read_csv(f\"../TADDY/data/for_TGN/{data_set}_times.csv\", sep=',').to_numpy()\n",
    "vertexs = pd.read_csv(f\"../TADDY/data/for_TGN/{data_set}_vertexs.csv\", sep=',').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b9d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anom_001 = np.where(data_test_001.T[2] == 1)[0]\n",
    "idx_anom_005 = np.where(data_test_005.T[2] == 1)[0]\n",
    "idx_anom_01 = np.where(data_test_01.T[2] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9423e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_anom_001 = data_times[len(data_train)+idx_anom_001-np.arange(len(idx_anom_001))]\n",
    "times_anom_005 = data_times[len(data_train)+idx_anom_005-np.arange(len(idx_anom_005))]\n",
    "tmp_ah = (len(data_train)+idx_anom_01-np.arange(len(idx_anom_01)))\n",
    "tmp_ah[-2] -= 1\n",
    "tmp_ah[-1] -= 1\n",
    "times_anom_01 = data_times[tmp_ah]\n",
    "\n",
    "#node_max = int(max(max(data_final.T[0]), max(data_final.T[1])))\n",
    "\n",
    "data_times_with_anom_001 = np.zeros((len(data_times)+len(idx_anom_001), 1))\n",
    "data_times_with_anom_001[:len(data_train)] = data_times[:len(data_train)]\n",
    "data_times_with_anom_001[len(data_train)+idx_anom_001] = times_anom_001\n",
    "data_times_with_anom_001[np.where(data_times_with_anom_001==0)[0]] = data_times[len(data_train):]\n",
    "\n",
    "data_times_with_anom_005 = np.zeros((len(data_times)+len(idx_anom_005), 1))\n",
    "data_times_with_anom_005[:len(data_train)] = data_times[:len(data_train)]\n",
    "data_times_with_anom_005[len(data_train)+idx_anom_005] = times_anom_005\n",
    "data_times_with_anom_005[np.where(data_times_with_anom_005==0)[0]] = data_times[len(data_train):]\n",
    "\n",
    "data_times_with_anom_01 = np.zeros((len(data_times)+len(idx_anom_01), 1))\n",
    "data_times_with_anom_01[:len(data_train)] = data_times[:len(data_train)]\n",
    "data_times_with_anom_01[len(data_train)+idx_anom_01] = times_anom_01\n",
    "data_times_with_anom_01[np.where(data_times_with_anom_01==0)[0]] = data_times[len(data_train):]\n",
    "\n",
    "data_final_001 = np.hstack((np.vstack((data_train, data_test_001[:,0:2])), data_times_with_anom_001))\n",
    "data_final_005 = np.hstack((np.vstack((data_train, data_test_005[:,0:2])), data_times_with_anom_005))\n",
    "data_final_01 = np.hstack((np.vstack((data_train, data_test_01[:,0:2])), data_times_with_anom_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c750fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last training instant for this dataset is:  57711600.0\n",
      "The last training instant for this dataset is:  57711600.0\n",
      "The last training instant for this dataset is:  57711600.0\n"
     ]
    }
   ],
   "source": [
    "data_final_001_nop = np.vstack((np.hstack((data[:, 0:2], data[:,3:])),\n",
    "                    np.hstack((vertexs[data_test_001[idx_anom_001][:, 0:2]].reshape((len(idx_anom_001), 2)), times_anom_001))))\n",
    "data_final_005_nop = np.vstack((np.hstack((data[:, 0:2], data[:,3:])),\n",
    "                    np.hstack((vertexs[data_test_005[idx_anom_005][:, 0:2]].reshape((len(idx_anom_005), 2)), times_anom_005))))\n",
    "data_final_01_nop = np.vstack((np.hstack((data[:, 0:2], data[:,3:])),\n",
    "                    np.hstack((vertexs[data_test_01[idx_anom_01][:, 0:2]].reshape((len(idx_anom_01), 2)), times_anom_01))))\n",
    "\n",
    "data_final_001_nop, anom_edges_001_nop = nop_TADDY(data_final_001_nop, len(idx_anom_001))\n",
    "data_final_005_nop, anom_edges_005_nop = nop_TADDY(data_final_005_nop, len(idx_anom_005))\n",
    "data_final_01_nop, anom_edges_01_nop = nop_TADDY(data_final_01_nop, len(idx_anom_01))\n",
    "\n",
    "Path('./data/anom_edges/').mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame(anom_edges_001_nop).to_csv(f'./data/anom_edges/{data_set}_TADDY_001_nop_anom_edges.csv',index=False)\n",
    "pd.DataFrame(anom_edges_005_nop).to_csv(f'./data/anom_edges/{data_set}_TADDY_005_nop_anom_edges.csv',index=False)\n",
    "pd.DataFrame(anom_edges_01_nop).to_csv(f'./data/anom_edges/{data_set}_TADDY_01_nop_anom_edges.csv',index=False)\n",
    "\n",
    "pd.DataFrame(data_final_001_nop).to_csv(f'./data/{data_set}_TADDY_001_nop.csv',index=False)\n",
    "pd.DataFrame(data_final_005_nop).to_csv(f'./data/{data_set}_TADDY_005_nop.csv',index=False)\n",
    "pd.DataFrame(data_final_01_nop).to_csv(f'./data/{data_set}_TADDY_01_nop.csv',index=False)\n",
    "\n",
    "# Indices des snaps de test (pour les données adaptées à TGN)\n",
    "\n",
    "ind_snaps_001 = snaps_final_indices(data_final_001, data_final_001_nop, snap_indices_001, data_train)\n",
    "ind_snaps_005 = snaps_final_indices(data_final_005, data_final_005_nop, snap_indices_005, data_train)\n",
    "ind_snaps_01 = snaps_final_indices(data_final_01, data_final_01_nop, snap_indices_01, data_train)\n",
    "\n",
    "Path('./data/snaps/').mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame(ind_snaps_001).to_csv(f'./data/snaps/{data_set}_TADDY_001_nop_snaps.csv',index=False)\n",
    "pd.DataFrame(ind_snaps_005).to_csv(f'./data/snaps/{data_set}_TADDY_005_nop_snaps.csv',index=False)\n",
    "pd.DataFrame(ind_snaps_01).to_csv(f'./data/snaps/{data_set}_TADDY_01_nop_snaps.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d43567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
